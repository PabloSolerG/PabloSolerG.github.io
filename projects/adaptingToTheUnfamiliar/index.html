<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Adapting to the Unfamiliar | Pablo Soler Garcia </title> <meta name="author" content="Pablo Soler Garcia"> <meta name="description" content="Leveraging Latent Space in Reinforcement Learning for Cross-Environment Generalization"> <meta name="keywords" content="robotics, electrical-engineering, machine-learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pablosolerg.github.io/projects/adaptingToTheUnfamiliar/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pablo</span> Soler Garcia </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Adapting to the Unfamiliar</h1> <p class="post-description">Leveraging Latent Space in Reinforcement Learning for Cross-Environment Generalization</p> </header> <article> <p><img src="/../assets/img/DLProject/header.gif" alt="Overview" style="width: 100%;"></p> <hr> <p>This project is part of the course Deep Learning. Adapting to the Unfamiliar! This project explores how reinforcement learning (RL) agents can leverage pre-trained world models to accelerate adaptation to new environments. By training DreamerV2 on multiple Atari game environments, we built a latent space dynamics model capturing the fundamental features of these environments. Our approach improved learning speed and performance in novel, unseen environments. Our findings show that using pre-trained world models offers significant advantages in speed and early performance during RL training. Fine-tuning all components of the model (encoder, decoder, and latent space) is critical for achieving these benefits. However, the limited number of environments and training steps in this project constrained the generalization capabilities of our model.</p> <p>üî∏ Authors: Jona Schulz, Lukas von Briel, Pablo Soler</p> <p> For more details, check out our report <a href="/../assets/pdf/DLProject.pdf" target="_blank">here</a>.</p> <hr> <h3 id="introduction">Introduction</h3> <p>Reinforcement learning often struggles to adapt quickly to new tasks. Inspired by how humans generalize knowledge, this project explores world models to encode the dynamics of multiple environments into a shared latent space. By fine-tuning these pre-trained models, we enabled faster and more efficient learning of RL agents in new settings. Our main research questions:</p> <ul> <li>Does a pre-trained world model improve RL training speed in novel environments?</li> <li>How does a world model encode shared properties across different environments?</li> </ul> <hr> <h3 id="methods">Methods</h3> <p>We conducted experiments in two setups:</p> <ul> <li>MinAtar Environments: Simplified 10x10 pixel versions of Atari games (e.g., Breakout, Asterix) for computational efficiency.</li> <li>Atari Environments: Full-scale Atari games from OpenAI Gym for more complex dynamics.</li> </ul> <p>Our project was based on the DreamerV2 architecture, which combines a world model and an actor-critic module. The World model encodes observations into a latent space and predicts future states. This way the model learns the dynamics of the environment to predict the next frame. By doing so it not only gains a deeper understanding of its surroundings, but it also enables itself to train with ‚Äúdreamed‚Äù environments without having to physically interact with the environment. The Actor-Critic model trains the RL agent using dreamed trajectories.</p> <div style="text-align: center;margin-bottom: 20px"> <img src="/assets/img/DLProject/predictions.gif" alt="Dreamed States" style="width: 90%; max-width: 800px;"> </div> <p>To adapt DreamerV2 for MinAtar, we modified the observation encoder and decoder to handle the smaller 10x10 input size. These can be seen in the following image.</p> <div style="text-align: center;margin-bottom: 20px"> <img src="/assets/img/DLProject/model_alterations.png" alt="Encoder-Decoder" style="width: 100%; max-width: 800px;"> </div> <h4 id="training-procedure">Training Procedure</h4> <p>Our training process involved three stages:</p> <ul> <li>Dataset Acquisition: Collecting data from multiple environments using DQN-based agents.</li> <li>World Model Pre-Training: Training DreamerV2‚Äôs world model on the collected dataset.</li> <li>Fine-Tuning: Adapting the pre-trained model to a novel environment.</li> </ul> <hr> <h3 id="results">Results</h3> <h4 id="minatar-experiments">MinAtar Experiments</h4> <p>We trained DreamerV2 on four MinAtar games and fine-tuned it on a novel environment, Space Invaders (all with similar controls). The pre-trained world model accelerated learning, as shown in the loss comparison:</p> <div style="text-align: center;"> <img src="/assets/img/DLProject/minAtar_loss.png" alt="minAtar_loss" style="width: 70%; max-width: 800px;"> <p style="font-size: 14px; color: gray;">World model loss comparison of a DreamerV2 agent with pre-trained world model with and without a fixed RSSM and an agent trained from scratch on MinAtar Space Invaders.</p> </div> <p>In full-scale Atari games, our pre-trained model demonstrated faster convergence during the initial training stages on the Breakout environment.</p> <div style="text-align: center;"> <img src="/assets/img/DLProject/atari_return.png" alt="minAtar_loss" style="width: 70%; max-width: 800px;"> <p style="font-size: 14px; color: gray;">Reward per episode comparison of a pre-trained and a non-pretrained DreamerV2 agent during fine-tuning on Breakout.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2024 Pablo Soler Garcia. Last updated: November 19, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>